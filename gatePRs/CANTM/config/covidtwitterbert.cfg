[BERT]
trainable_layers = encoder.layer.23.attention.self.query.weight,encoder.layer.23.attention.self.query.bias,encoder.layer.23.attention.self.key.weight,encoder.layer.23.attention.self.key.bias,encoder.layer.23.attention.self.value.weight,encoder.layer.23.attention.self.value.bias,encoder.layer.23.attention.output.dense.weight,encoder.layer.23.attention.output.dense.bias,encoder.layer.23.intermediate.dense.weight,encoder.layer.23.intermediate.dense.bias,encoder.layer.23.output.dense.weight,encoder.layer.23.output.dense.bias
bert_path = digitalepidemiologylab/covid-twitter-bert-v2
bert_dim = 1024

[MODEL]
dynamic_sample = yes
weight_loss = no
n_samples = 1

[READER_POSTPROCESSOR]
max_sent_len = 400

[GENSIM_DICT]
no_below = 1
no_above = 0.7
keep_n = 3000
